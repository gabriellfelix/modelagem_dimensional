# üöÄ Desafio T√©cnico - Engenheiro de Dados J√∫nior

## üßæ Descri√ß√£o
Este projeto implementa uma modelagem dimensional (esquema estrela) a partir de uma tabela √∫nica de dados brutos de vendas, produtos e clientes. A transforma√ß√£o foi realizada utilizando PySpark para processar os dados, criar as tabelas de dimens√£o e a tabela fato, permitindo an√°lises eficientes e escal√°veis.

## üìÅ Estrutura dos Dados

**Arquivo de entrada:** `dados_brutos.csv`

**Colunas dispon√≠veis:**
- nome_cliente
- cidade
- estado
- nome_produto
- categoria
- fabricante
- data
- qtd_vendida
- valor_total

## üèó Modelagem Criada

### Tabelas de Dimens√£o:

- **dim_cliente**  
  Cont√©m informa√ß√µes √∫nicas sobre os clientes.
  - id_cliente (surrogate key)
  - nome_cliente

- **dim_produto**  
  Armazena detalhes sobre os produtos vendidos.
  - id_produto (surrogate key)
  - nome_produto
  - nome_categoria
  - nome_fabricante

- **dim_data**  
  Derivada da coluna `data`, com decomposi√ß√£o em componentes temporais.
  - id_data (surrogate key)
  - data
  - ano
  - mes
  - dia
  - dia_semana

- **dim_local**  
  Cont√©m informa√ß√µes geogr√°ficas relacionadas √†s vendas.
  - id_local (surrogate key)
  - nome_cidade
  - nome_estado

### Tabela Fato:

- **fato_vendas**  
  Tabela principal contendo as medidas de neg√≥cio e chaves para as dimens√µes.
  - id_venda (surrogate key)
  - id_cliente (foreign key)
  - id_produto (foreign key)
  - id_local (foreign key)
  - id_data (foreign key)
  - qtd_vendida (medida)
  - valor_total (medida)

## üõ† Tecnologias Utilizadas

- **PySpark**: Framework de processamento distribu√≠do para transforma√ß√£o de dados em larga escala
- **Spark SQL**: Para consultas aos dados
- **Python**: Linguagem de programa√ß√£o principal
- **Jupyter Notebook**: Ambiente de desenvolvimento interativo
- **Git e GitHub**: Para Versionamento

## üìÇ Arquivos no Reposit√≥rio

- `Modelagem Dimensional.ipynb`: Notebook com todo o c√≥digo de transforma√ß√£o e modelagem
- `dados/entrada/dados_brutos.csv`: Arquivo com os dados brutos de entrada
- `dados/saida/`: Diret√≥rio contendo as tabelas dimensionais e fato exportadas
  - `dim_cliente.csv`
  - `dim_produto.csv`
  - `dim_data.csv`
  - `dim_local.csv`
  - `fato_vendas.csv`
- `diagrama.png`: Representa√ß√£o visual do modelo dimensional implementado

## ‚ñ∂Ô∏è Como Executar

1. Clone este reposit√≥rio para sua m√°quina local
2. Instale as depend√™ncias necess√°rias:
   ```
   pip install pyspark jupyter
   ```
3. Execute o notebook Jupyter:
   ```
   jupyter notebook "Modelagem Dimensional.ipynb"
   ```
4. Os resultados ser√£o salvos automaticamente no diret√≥rio `dados/saida/`

## üìä Decis√µes de Modelagem

1. **Chaves substitutas (surrogate keys)**: Utilizei `monotonically_increasing_id()` para gerar chaves √∫nicas em todas as dimens√µes, garantindo integridade referencial e melhor desempenho em consultas.

2. **Normaliza√ß√£o de dados**: Implementei uma padroniza√ß√£o no casing dos campos de texto (`initcap` para nomes, `upper` para siglas) para evitar duplicidades nas dimens√µes.

3. **Dimens√£o data**: Extra√≠ componentes temporais (ano, m√™s, dia, dia da semana) para facilitar an√°lises por per√≠odo e sazonalidade.

4. **Dimens√£o local**: Agrupei cidade e estado em uma √∫nica dimens√£o, seguindo o padr√£o geogr√°fico de hierarquia natural.

5. **Tabela fato**: Mantive a granularidade no n√≠vel de transa√ß√£o individual de venda, preservando as medidas de quantidade e valor total.

6. **Valida√ß√£o de qualidade**: Implementei verifica√ß√µes de valores nulos e inconsist√™ncias antes da cria√ß√£o do modelo dimensional.